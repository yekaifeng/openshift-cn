apiVersion: v1
data:
  fluent.conf: |+
    # This file is the fluentd configuration entrypoint. Edit with care.
    # This CM is used with image kennethye/ose-logging-fluentd:v4.2.11-201912100122-kafka
    # It is the example of forwarding to kafka 0.10 cluster with sasl plaintext
    @include configs.d/openshift/system.conf

    # In each section below, pre- and post- includes don't include anything initially;
    # they exist to enable future additions to openshift conf as needed.

    ## sources
    ## ordered so that syslog always runs last...
    @include configs.d/openshift/input-pre-*.conf
    @include configs.d/dynamic/input-docker-*.conf
    @include configs.d/dynamic/input-syslog-*.conf
    @include configs.d/openshift/input-post-*.conf
    ##

    <label @INGRESS>
    ## filters
      @include configs.d/openshift/filter-pre-*.conf
      @include configs.d/openshift/filter-retag-journal.conf
      @include configs.d/openshift/filter-k8s-meta.conf
      @include configs.d/openshift/filter-kibana-transform.conf
      @include configs.d/openshift/filter-k8s-flatten-hash.conf
      @include configs.d/openshift/filter-k8s-record-transform.conf
      @include configs.d/openshift/filter-syslog-record-transform.conf
      @include configs.d/openshift/filter-viaq-data-model.conf
      @include configs.d/openshift/filter-post-*.conf
     ##
    </label>

    <label @OUTPUT>
    ## matches
       @include configs.d/openshift/output-pre-*.conf
       @include configs.d/openshift/output-operations.conf
       @include configs.d/openshift/output-applications.conf
       # no post - applications.conf matches everything left
    ##
    </label>

  output-extra-file.conf: |-
    #<store>
    #    @type file
    #    path /var/log/fluentd/all-logs
    #</store>
  output-extra-kafka-buffered.conf: |-
    <store>
          @type              kafka2

          # Brokers: you can choose either brokers or zookeeper.
          brokers             apache-kafka2.kafka.svc:9092 # Set brokers directly
    #      topic_key           logging
    #      partition_key         (string) :default => 'partition'
    #      partition_key_key     (string) :default => 'partition_key'
    #      message_key_key       (string) :default => 'message_key'
          default_topic       logging
    #      default_partition_key (string) :default => nil
    #      default_message_key   (string) :default => nil
          <format>
            @type json
          </format>
          <buffer topic>
            flush_interval 10s
          </buffer>
          max_send_retries    1
          required_acks       -1
          username            admin
          password            12345
    </store>
  output-extra-kafka.conf: |-
    #<store>
    #@type kafka

      # Brokers: you can choose either brokers or zookeeper.
    #  brokers             apache-kafka2.kafka.svc:9092 # Set brokers directly
    #  default_topic       logging
      # default_partition_key (string)   :default => nil
    #  output_data_type    json
    #    username            admin
    #    password            12345
    #</store>
  secure-forward.conf: |-
    # <store>
    # @type secure_forward

    # self_hostname ${hostname}
    # shared_key <SECRET_STRING>

    # secure yes
    # enable_strict_verification yes

    # ca_cert_path /etc/fluent/keys/your_ca_cert
    # ca_private_key_path /etc/fluent/keys/your_private_key
      # for private CA secret key
    # ca_private_key_passphrase passphrase

    # <server>
      # or IP
    #   host server.fqdn.example.com
    #   port 24284
    # </server>
    # <server>
      # ip address to connect
    #   host 203.0.113.8
      # specify hostlabel for FQDN verification if ipaddress is used for host
    #   hostlabel server.fqdn.example.com
    # </server>
    # </store>
    #<store>
    #  @type relabel
    #  @label @HTTP
    #</store>
  throttle-config.yaml: |
    # Logging example fluentd throttling config file

    #example-project:
    #  read_lines_limit: 10
    #
    #.operations:
    #  read_lines_limit: 100
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: logging-fluentd-kafka-sasl
